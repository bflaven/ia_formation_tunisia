{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQHMIPoDNn4C"
   },
   "source": [
    "## Comment transcrire automatiquement avec Whisper\n",
    "\n",
    "### Étape 1 : Installer Google Drive et définir le chemin d'accès au fichier\n",
    "\n",
    "Cette étape installe votre Google Drive pour accéder au fichier audio.\n",
    "Assurez-vous que le répertoire « source » et le fichier \"en_sample_0.mp3\" existent dans le dossier Google Drive spécifié.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2615,
     "status": "ok",
     "timestamp": 1744197962461,
     "user": {
      "displayName": "Bruno Flaven",
      "userId": "05777293442111480888"
     },
     "user_tz": -120
    },
    "id": "MOlxbzWFNtA6",
    "outputId": "163b20cd-589b-4292-e9e0-a1c58d08ec6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Audio file found at '/content/drive/MyDrive/tunisie_formation/source/sp_sample_1.mp3'. Proceeding...\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "# require for filename\n",
    "import datetime\n",
    "\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# to attempt to forcibly remount\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "# Define the Google Drive folder ID and the audio file name\n",
    "gdrive_folder_id = \"[add your folder id here]\"\n",
    "\n",
    "# select the file\n",
    "# audio_file_name = \"en_sample_0.mp3\"\n",
    "audio_file_name = \"sp_sample_1.mp3\"\n",
    "\n",
    "\n",
    "# concatenate the filename to the transcription\n",
    "audio_file_name_without_ext = os.path.splitext(audio_file_name)[0]\n",
    "\n",
    "# Construct the full path to the audio file in Google Drive\n",
    "audio_file_path = f\"/content/drive/MyDrive/tunisie_formation/source/{audio_file_name}\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(audio_file_path):\n",
    "    print(f\"Error: Audio file not found at '{audio_file_path}'.\")\n",
    "else:\n",
    "    print(f\"Audio file found at '{audio_file_path}'. Proceeding...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdnBHVcTN0yv"
   },
   "source": [
    "### Étape 2 : Installer Whisper et charger le petit modèle\n",
    "\n",
    "Cette étape installe la bibliothèque Whisper depuis GitHub et charge le « petit » modèle pré-entraîné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16840,
     "status": "ok",
     "timestamp": 1744198007212,
     "user": {
      "displayName": "Bruno Flaven",
      "userId": "05777293442111480888"
     },
     "user_tz": -120
    },
    "id": "zfc7jwbmN6jS",
    "outputId": "d4beb15d-bc69-4a93-c086-2b363ae3cc42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-wwiqiigw\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-wwiqiigw\n",
      "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.6.0)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.9.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
      "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
      "\n",
      "--- Whisper 'small' model loaded successfully ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git\n",
    "import whisper\n",
    "\n",
    "# Load the small Whisper model\n",
    "model = whisper.load_model(\"small\")\n",
    "print(\"\\n--- Whisper 'small' model loaded successfully ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NhDaXICONH_"
   },
   "source": [
    "### Étape 3 : Transcrire l'audio avec la détection de la langue\n",
    "\n",
    "Cette étape transcrit le fichier audio à l'aide du modèle Whisper chargé.\n",
    "Il détecte automatiquement la langue et fournit la probabilité de la langue détectée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77084,
     "status": "ok",
     "timestamp": 1744198173912,
     "user": {
      "displayName": "Bruno Flaven",
      "userId": "05777293442111480888"
     },
     "user_tz": -120
    },
    "id": "Kp9gHiH0OSGH",
    "outputId": "e5752efe-5187-420c-9b61-d8bb6776a5aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting transcription...\n",
      "Transcription completed.\n",
      "Detected language: es\n",
      "\n",
      "---Transcription:---\n",
      "\n",
      " Vivíamos en el campo en la zona de las arenas que allí estaba la uva buena. Allí sacábamos nosotros el vino. Yo tendría en esos tiempos 10 o 15 años por ahí. Bueno, nosotros teníamos una par de hectáreas de viñedo, digamos, de bien, de parras, y recogíamos esas uvas. Después la pasábamos a pisarla con los pies, porque recogíamos ese cardo y lo echábamos en unos recipientes y lo metíamos en un sitio oscuro y fresco y fermentaba. Una vez que estaba fermentado a los 5 meses ya estaba hecho mosto, vino. El arcoe que te hace fermentada, ¿vale? El grado de azúcar que tiene la uva. Mientras más grado tenga azúcar, más pronto fermenta. Pero estaba riquísimo y también sacábamos arrope. El arrope sabe lo que es. Pues herví el cardo del mosto ya cuando se pisa la uva. Se echaba en unos carderos, se le pone fuego y se reduce hasta que se pone ya el yó azúcar. Y esa reducción se queda cardoso y es una arrope que les echábamos por encima las tostadas. Y está riquísimo. Además ha perdido aquí en el pueblo, no sé por qué.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(audio_file_path):\n",
    "    print(\"\\nStarting transcription...\")\n",
    "    # Transcribe the audio file\n",
    "    # result = model.transcribe(audio_file_path)\n",
    "    # correction for error message \"FP16 is not supported on CPU; using FP32 instead\"\n",
    "    result = model.transcribe(audio_file_path, fp16=False)\n",
    "    print(\"Transcription completed.\")\n",
    "\n",
    "    # Print the detected language\n",
    "    print(f\"Detected language: {result['language']}\")\n",
    "\n",
    "    # The 'language_probability' key is no longer available in newer Whisper versions.\n",
    "    # This line is removed to avoid the KeyError.\n",
    "    #print(f\"Language probability: {result['language_probability']:.4f}\")\n",
    "\n",
    "    # Store the transcribed text\n",
    "    transcribed_text = result[\"text\"]\n",
    "    print(\"\\n---Transcription:---\\n\")\n",
    "    print(transcribed_text)\n",
    "else:\n",
    "    transcribed_text = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtVj7r3FOgeP"
   },
   "source": [
    "### Étape 4 : Afficher le résultat de la transcription et activer le téléchargement dans différents formats\n",
    "\n",
    "Cette étape affiche le résultat de la transcription et propose des boutons pour le télécharger aux formats .srt, .csv et .txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1744198247677,
     "user": {
      "displayName": "Bruno Flaven",
      "userId": "05777293442111480888"
     },
     "user_tz": -120
    },
    "id": "_quDcA3rOrLX",
    "outputId": "088c178f-e3d6-419d-d5cc-08ae1b98b9f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Download Options ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_ceffa2ff-9117-4e82-ab2f-f96ea7fa71e7\", \"transcription_sp_sample_1_20250409_113047.txt\", 0)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Download succeeded---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt_content = transcribed_text\n",
    "from google.colab import files\n",
    "\n",
    "# V1\n",
    "# if transcribed_text is not None:\n",
    "#     print(\"\\n--- Download Options ---\")\n",
    "#     # Download as TXT\n",
    "#     with open(\"transcription.txt\", \"w\") as f:\n",
    "#       f.write(txt_content)\n",
    "#       files.download(\"transcription.txt\")\n",
    "#       print('Download succeeded')\n",
    "\n",
    "\n",
    "# V2\n",
    "# if transcribed_text is not None:\n",
    "#     print(\"\\n--- Download Options ---\")\n",
    "#     # Download as TXT\n",
    "#     transcription_filename = f\"transcription_{audio_file_name_without_ext}.txt\"  # New filename\n",
    "#     with open(transcription_filename, \"w\") as f:\n",
    "#         f.write(txt_content)\n",
    "#         files.download(transcription_filename)  # Use the new filename\n",
    "#         print('Download succeeded')\n",
    "\n",
    "\n",
    "\n",
    "# V3\n",
    "if transcribed_text is not None:\n",
    "    print(\"\\n--- Download Options ---\\n\")\n",
    "    # Get current timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # Create filename with audio prefix and timestamp\n",
    "    transcription_filename = f\"transcription_{audio_file_name_without_ext}_{timestamp}.txt\"\n",
    "    with open(transcription_filename, \"w\") as f:\n",
    "        # Concatenate audio filename and transcription\n",
    "        f.write(f\"Audio file: {audio_file_name}\\n\\n\")  # Add audio filename\n",
    "        f.write(txt_content)  # Add transcription content\n",
    "        files.download(transcription_filename)\n",
    "        print(\"\\n---Download succeeded---\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNz0COfRCqTr5U0J/s4cLMh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
